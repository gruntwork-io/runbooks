---
alwaysApply: true
---

The Go server runs on port 7825 by default.

We use bun, so prefer that to npm.

We use https://taskfile.dev/ over makefiles. Look for the Taskfile.yml to see what command can you can run.

Docs are located in /docs 

The frontend is located in /web 

The backend is spread across /api, /browser, and /cmd. When the user requests something, make sure you know if they're asking to update the docs, frontend, or backend.

Whenever you plan to use Terraform examples, use OpenTofu instead.

## Error Reporting in MDX Components

The `useErrorReporting()` hook and `reportError()` function are for **configuration errors only** - problems with how the runbook MDX is written. These trigger the "This runbook has issues" banner.

**DO report** (configuration errors):
- Duplicate component IDs
- Missing required props
- Invalid prop values/combinations
- Malformed configurations

**DO NOT report** (runtime/operational errors):
- Authentication failures (bad credentials, expired tokens)
- Network errors during API calls
- AWS/cloud provider API errors
- User input validation failures during execution

Runtime errors should be displayed inline within the component (e.g., with an alert box) but should NOT call `reportError()`. The distinction: configuration errors are problems with the runbook itself; runtime errors are problems that occur when a user interacts with a correctly-configured runbook.

# Writing automated tests

Wherever possible, use the "source of truth" to recreate the program behavior, versus manually re-implementing the same logic in the test. For example, do not re-implement logic in the testing framework if you can avoid it. Instead, reference the actual codebase, and if necessary re-factor that codebase into public functions to call those functions as needed.

When you encounter a test failure, assume that the problem is with the runbooks configuration. If that appears to be correct, check the runbooks codebase itself. Only consider updating the test framework if the test framework does not faithfully reproduce the results the "real" runbooks codebase uses. Otherwise, be very hesitant to update the Runbooks test code. The goal is to make sure both the tested runbooks are correct and that the runbook codebase has not had regressions, so we don't want to "make the tests pass" by updating the test code to detect an otherwise legitimate failure.

Whenever you create a new runbook, create an automated test to validate that the runbook works. Following the testing guidelines at docs/src/content/docs/authoring/testing.mdx. Start by creating a test configuration file with `runbooks test init /path/to/runbook`, and then run the automated test with `runbooks test /path/to/runbook`.

If the test has integration tests because it has third-party dependencies, skip testing those. But if it has locally run scripts, do test those. If the test fails, resolve the issue, and re-run the test to see if you've fixed the issue.

# Creating new blocks

Whenever you're defining a new block, be sure to add not only the frontend/backedn code for the block itself, but also docs, automated tests, and support in the runbooks testing framework.

For the Runbooks testing framework, `runbooks init` should detect the new block, and `runbooks test` should take thoughtful steps when the block is executed.