---
title: Testing Runbooks
description: How to write and run automated tests for your runbooks.
---

import { Tabs, TabItem } from '@astrojs/starlight/components';

Runbooks includes a built-in testing framework that lets you validate that your runbooks work correctly.

To run a test, you define a YAML test configuration file alongside your runbook. Then call `runbooks test /path/to/runbook` or `runbooks test ./...` to run the tests. Test are meant to run locally or in CI.

## Quick Start

1. Generate a test configuration for your runbook:

```bash
runbooks test init ./my-runbook
```

This creates `runbook_test.yml` with reasonable defaults based on your runbook's blocks. You can edit the file to customize the tests if needed.

2. Run the tests:

```bash
runbooks test ./my-runbook
```

## Test Configuration

Tests are defined in a `runbook_test.yml` file located in the same directory as your `runbook.mdx` file. Here's an example of a test configuration file:

```yaml
version: 1

settings:
  # use_temp_working_dir: true  # Default: use isolated temp directory
  # working_dir: .              # Alternative: use runbook's directory
  # output_path: generated      # Where to write generated files (relative to working_dir)
  timeout: 5m            # Test timeout
  parallelizable: true   # Can run in parallel with other runbooks

tests:
  - name: happy-path
    description: Standard successful execution
    
    inputs:
      project.Name: "test-project"
      project.Language: "python"
    
    steps:
      - block: check-requirements
        expect: success
      
      - block: create-project
        expect: success
        outputs: [project_id]
    
    assertions:
      - type: file_exists
        path: generated/README.md
      
      - type: file_contains
        path: generated/README.md
        contains: "test-project"
    
    cleanup:
      - command: rm -rf /tmp/test-resources
```

## Test Structure

### Settings

Global settings that apply to all tests in the file:

| Setting | Default | Description |
|---------|---------|-------------|
| `use_temp_working_dir` | `true` | Use a temporary working directory (automatically cleaned up). Overrides `working_dir` if set. |
| `working_dir` | current directory | Base directory for script execution and file generation. Use `.` for the runbook's directory. |
| `output_path` | `generated` | Directory where generated files are written, relative to the working directory. |
| `timeout` | `5m` | Maximum time for each test case |
| `parallelizable` | `true` | Whether this runbook's tests can run in parallel with others |

#### Working Directory Behavior

The working directory determines where scripts execute and where generated files are written:

- **Default**: Uses a temporary directory (`use_temp_working_dir: true`) for isolated test runs
- **Custom directory**: Set `working_dir` to a specific path and `use_temp_working_dir: false`
- **Output path**: Generated files go to `{working_dir}/{output_path}` (e.g., `/tmp/runbook-xyz/generated`)

Example with custom working directory:

```yaml
settings:
  use_temp_working_dir: false
  working_dir: /tmp/my-test-output
  output_path: generated
```

### Test Cases

Each test case has:

- **name** (required): Unique identifier for the test
- **description**: Human-readable description
- **env**: Environment variables to set for all blocks in this test
- **inputs**: Variable values for the test
- **steps**: Blocks to execute (in order)
- **assertions**: Validations to run after all steps
- **cleanup**: Actions to run after the test (even on failure)

### Environment Variables

Set environment variables for a test case using the `env` field:

```yaml
tests:
  - name: dry-run-test
    description: Test with dry-run mode enabled
    
    env:
      RUNBOOK_DRY_RUN: "true"
      MY_API_KEY: "test-key"
    
    steps:
      - block: create-resource
        expect: success
```

Environment variables are available to all blocks executed in the test case.

### Steps

Each step specifies a block to execute and the expected result:

```yaml
steps:
  - block: check-git
    expect: success

  - block: create-resources
    expect: success
    outputs: [resource_id, resource_name]
```

If you don't specify any blocks, the test will run all blocks in the runbook in document order.

In your test configuration file, you can specify the expected status of a block:

| Status | Description |
|--------|-------------|
| `success` | Block completed successfully (exit code 0) |
| `fail` | Block failed (exit code 1+) |
| `warn` | Block completed with warning (exit code 2) |
| `blocked` | Block should be blocked due to missing dependencies |
| `skip` | Skip this block (useful for AwsAuth in non-AWS tests) |

### Per-Step Assertions

You can run assertions immediately after a step completes:

```yaml
steps:
  - block: create-resource
    expect: success
    assertions:
      - type: output_exists
        output: resource_id
```

## Assertions

Assertions validate the test results. They run after all steps complete (or immediately after a step if defined inline as we just saw above).

### File Assertions

The following file assertions are useful for validating the content of generated files:

```yaml
assertions:
  # Check file exists
  - type: file_exists
    path: generated/README.md

  # Check file does not exist
  - type: file_not_exists
    path: generated/temp.txt

  # Check file contains substring
  - type: file_contains
    path: generated/README.md
    contains: "My Project"

  # Check file does not contain substring
  - type: file_not_contains
    path: generated/README.md
    contains: "TODO"

  # Check file matches regex pattern
  - type: file_matches
    path: generated/config.json
    pattern: '"version":\s*"\d+\.\d+\.\d+"'

  # Check file equals exact content
  - type: file_equals
    path: generated/version.txt
    value: "1.0.0"

  # Check template generated files
  - type: files_generated
    block: my-template
    min_count: 1
```

### Directory Assertions

The following folder assertions are useful for validating the status of generated folders:

```yaml
assertions:
  - type: dir_exists
    path: generated/src

  - type: dir_not_exists
    path: generated/temp
```

### Output Assertions

The following assertions are useful for validating outputs of blocks:

```yaml
assertions:
  # Check output equals value
  - type: output_equals
    block: create-resource
    output: status
    value: "created"

  # Check output matches pattern
  - type: output_matches
    block: create-resource
    output: resource_id
    pattern: "^[a-f0-9]{12}$"

  # Check output exists
  - type: output_exists
    block: create-resource
    output: resource_id
```

### Script Assertions

The following script assertions are useful to run any custom validation logic:

```yaml
assertions:
  # Run custom script (exit 0 = pass)
  - type: script
    command: test -f generated/config.json && jq -e '.version' generated/config.json
```

## Inputs

Inputs are used to set the values templates, which are used to generate files, commands, and checks. 

Runbooks tests support both literal values and fuzz values for inputs.

### Literal Values

Literal values are used to set the values of inputs to specific values.

```yaml
inputs:
  project.Name: "test-project"
  project.Description: "A test project"
  config.Port: 8080
  config.Enabled: true
```

### Fuzz Values

Fuzz values are used to generate random values for testing. The test framework will generate a new random value that meets the given constraints for each test run.

```yaml
inputs:
  # Random alphanumeric string
  project.Name:
    fuzz: { type: alphanumeric, length: 15 }
  
  # Random words
  project.Description:
    fuzz: { type: words, length: 5 }
  
  # Random from enum options
  project.Language:
    fuzz: { type: enum }
  
  # Random UUID
  resource.ID:
    fuzz: { type: uuid }
  
  # Random integer
  config.Port:
    fuzz: { type: int, min: 3000, max: 9000 }
  
  # Random boolean
  config.Enabled:
    fuzz: { type: bool }
  
  # Random email
  user.Email:
    fuzz: { type: email }
  
  # Random URL
  config.Endpoint:
    fuzz: { type: url }
  
  # Random date (YYYY-MM-DD)
  project.StartDate:
    fuzz: { type: date }
  
  # Random timestamp (RFC3339)
  event.Timestamp:
    fuzz: { type: timestamp }
```

## Cleanup

Cleanup actions run after the test completes, even if the test fails:

```yaml
cleanup:
  # Inline command
  - command: rm -rf /tmp/test-resources

  # Script file
  - path: cleanup/teardown.sh
```

## Testing AwsAuth Blocks

For runbooks with `<AwsAuth>` blocks, use `prefilledCredentials={{ type: "env" }}` to enable headless testing:

```mdx
<AwsAuth
  id="aws-auth"
  prefilledCredentials={{ type: "env" }}
  allowOverridePrefilled={false}
/>
```
This will look specifically for the following environment variables:

- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN`
- `AWS_REGION`

Consider using the prefix `RUNBOOKS_TEST_` for environment variables that are used for testing. This will help you avoid conflicts with other environment variables. 

```mdx
<AwsAuth
  id="aws-auth"
  prefilledCredentials={{ type: "env", prefix: "RUNBOOKS_TEST_" }}
  allowOverridePrefilled={false}
/>
```
This will look specifically for the following environment variables:

- `RUNBOOKS_TEST_AWS_ACCESS_KEY_ID`
- `RUNBOOKS_TEST_AWS_SECRET_ACCESS_KEY`
- `RUNBOOKS_TEST_AWS_SESSION_TOKEN`
- `RUNBOOKS_TEST_AWS_REGION`

In CI, be sure to set the environment variables before running tests. The following example shows one way to set the environment variables for a GitHub Actions workflow:

```yaml
# .github/workflows/test.ymlls
  uses: aws-actions/configure-aws-credentials@v4
  with:
    role-to-assume: arn:aws:iam::123456789012:role/GitHubActionsRole
    aws-region: us-west-2

- name: Run runbook tests
  run: runbooks test ./runbooks/...
  env:
    RUNBOOK_TEST_AWS_ACCESS_KEY_ID: ${{ env.AWS_ACCESS_KEY_ID }}
    RUNBOOK_TEST_AWS_SECRET_ACCESS_KEY: ${{ env.AWS_SECRET_ACCESS_KEY }}
    RUNBOOK_TEST_AWS_SESSION_TOKEN: ${{ env.AWS_SESSION_TOKEN }}
    RUNBOOK_TEST_AWS_REGION: ${{ env.AWS_REGION }}
```

## Running Tests

When you run a runbook test, the test framework will:
- Run the runbook in a temporary directory
- Capture the output of any scripts or generated files
- Validate the output of the runbook against the assertions in your test configuration file, if any
- Return a status code of 0 if the test passed, 1 if the test failed, and 2 if the test was skipped

If a runbook test fails, you will see additional details about which block failed and why.

### Single Runbook

You can run a test on a single runbook.

```bash
# Run all tests for a runbook
runbooks test ./my-runbook

# Run a specific test case
runbooks test ./my-runbook --test happy-path

# Verbose output
runbooks test ./my-runbook -v
```

### Multiple Runbooks

Use the `...` glob pattern to discover and test all runbooks:

```bash
# Test all runbooks in a directory tree
runbooks test ./runbooks/...

# Control parallel execution
runbooks test ./runbooks/... --max-parallel 4
```

### CI Output

Generate JUnit XML for CI integration:

```bash
runbooks test ./runbooks/... --output junit --output-file results.xml
```

## Out-of-Order Testing

In some cases, for testing purposes, you may wish to execute blocks in an order different than how they're defined in the runbook. You can do this by explicitly specifying the order of the blocks in the test configuration file.

Keep in mind, though, that when new blocks are added to the runbook, they will not be included in the test unless you explicitly add them to the test configuration file.

That's why we recommend leaving the `steps` section empty in your test configuration file if possible. This will ensure that all future blocks are included in the test.

```yaml
tests:
  - name: out-of-order
    description: Test dependency enforcement
    
    steps:
      # This should be blocked because create-account hasn't run
      - block: create-resources
        expect: blocked
        missing_outputs:
          - _blocks.create_account.outputs.account_id

      # Run the dependency
      - block: create-account
        expect: success

      # Now this should succeed
      - block: create-resources
        expect: success
```

## Example Workflow

<Tabs>
  <TabItem label="GitHub Actions">
```yaml
# .github/workflows/test-runbooks.yml
name: Test Runbooks

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    steps:
      - uses: actions/checkout@v4
      
      - name: Set up runbooks
        run: |
          curl -sL https://github.com/gruntwork-io/runbooks/releases/latest/download/runbooks_linux_amd64 -o runbooks
          chmod +x runbooks
      
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ vars.AWS_ROLE_ARN }}
          aws-region: ${{ vars.AWS_REGION }}
      
      - name: Run runbook tests
        run: ./runbooks test ./runbooks/... --output junit --output-file results.xml
        env:
          # Map OIDC credentials to RUNBOOKS_TEST_ prefix
          RUNBOOKS_TEST_AWS_ACCESS_KEY_ID: ${{ env.AWS_ACCESS_KEY_ID }}
          RUNBOOKS_TEST_AWS_SECRET_ACCESS_KEY: ${{ env.AWS_SECRET_ACCESS_KEY }}
          RUNBOOKS_TEST_AWS_SESSION_TOKEN: ${{ env.AWS_SESSION_TOKEN }}
          RUNBOOKS_TEST_AWS_REGION: ${{ vars.AWS_REGION }}
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: results.xml
```
  </TabItem>
  <TabItem label="GitLab CI">
```yaml
# .gitlab-ci.yml
test-runbooks:
  stage: test
  variables:
    RUNBOOKS_TEST_AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID
    RUNBOOKS_TEST_AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY
    RUNBOOKS_TEST_AWS_REGION: $AWS_REGION
  script:
    - curl -sL https://github.com/gruntwork-io/runbooks/releases/latest/download/runbooks_linux_amd64 -o runbooks
    - chmod +x runbooks
    - ./runbooks test ./runbooks/... --output junit --output-file results.xml
  artifacts:
    reports:
      junit: results.xml
```
  </TabItem>
</Tabs>

## Testing with External Dependencies

Runbooks that interact with external services (GitHub, AWS, Terraform, etc.) require special testing strategies. We recommend a tiered approach.

As you ascend the tiers, you will gain more completeness at the expense of slower speed and more complexity. This builds on the concepts of the [traditional test pyramid](https://martinfowler.com/articles/practical-test-pyramid.html).

### Tier 1: Template Validation

Template validation tests that templates generate correct files without making any external calls.

```yaml
tests:
  - name: template-validation
    description: Validate template generation
    
    inputs:
      config.ProjectName: "test-project"
      config.Region: "us-east-1"
    
    steps:
      - block: generate-config  # Template block
        expect: success
      - block: deploy-resources  # Skip external calls
        expect: skip
    
    assertions:
      - type: files_generated
        block: generate-config
        min_count: 1
```

### Tier 2: Dry-Run Mode

Dry-run mode tests that script logic works correctly without making any external calls. They leverage the fact that the script was written to support a `RUNBOOK_DRY_RUN` environment variable.

Use the `RUNBOOK_DRY_RUN` environment variable pattern to test script logic without side effects. Note that the `RUNBOOK_DRY_RUN` environment variable is not special in any way, it is just a convention. You can use any environment variable you want to trigger the dry-run mode.

**In your Command or Check script:**

```bash
#!/bin/bash
set -e

# Dry-run support
DRY_RUN="${RUNBOOK_DRY_RUN:-false}"

if [[ "$DRY_RUN" == "true" ]]; then
    echo "[DRY-RUN] Would create PR in $GITHUB_ORG/$REPO_NAME"
    echo "[DRY-RUN] gh pr create --title '$TITLE'"
    exit 0
fi

# Real execution
gh pr create --title "$TITLE" --body "$BODY"
```

**In your test configuration file:**

```yaml
tests:
  - name: dry-run-flow
    description: Test full flow without real API calls
    
    env:
      RUNBOOK_DRY_RUN: "true"
    
    steps:
      - block: generate-config
        expect: success
      - block: create-pr
        expect: success  # Succeeds in dry-run mode
```

### Tier 3: Integration Tests

For full integration testing with real services, set environment variables for credentials in CI and run tests manually or on a schedule:

```yaml
tests:
  - name: integration-test
    description: Full integration (requires credentials)
    
    env:
      # Credentials set in CI environment will automatically be available to the test.
      # So no need to set them in the runbook_test.yml file.
    
    steps:
      - block: deploy-resources
        expect: success
    
    cleanup:
      - command: ./scripts/cleanup-test-resources.sh
```

## Best Practices

1. **Start with `test init`**: Generate your test configuration automatically, then customize it.

2. **Test happy paths first**: Ensure your runbook works when everything goes right.

3. **Test edge cases**: Add tests for expected failures, missing dependencies, and error handling.

4. **Use fuzz values**: Generate random inputs to catch edge cases you might not think of.

5. **Keep tests fast**: Use `use_temp_working_dir: true` and avoid slow external dependencies when possible.

6. **Clean up resources**: Use the `cleanup` section to remove any resources created during testing.

7. **Use CI**: Run tests on every pull request to catch issues early.

8. **Use tiered testing**: For runbooks with external dependencies, use dry-run mode in CI and save integration tests for manual/scheduled runs.

Finally, note that tests are not free to write or maintain. But it is generally worth the investment to know that your runbooks work as expected. Choose accordingly how much to invest in testing your runbooks.
